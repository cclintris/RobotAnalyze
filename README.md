[TOC]

<div style="page-break-after: always;"></div>

# 数据集成作业二 - 实验报告 - 12组

## 团队成员以及分工

| 团队成员               | 分工                                                         |
| :--------------------- | :----------------------------------------------------------- |
| 181250083 林希澄(组长) | A 部分关注点分析，前端数据可视化，分配任务，完善整理文档+报告 |
| 181250066 李镔达       | 数据库搭建，A 部分关注点分析                                 |
| 181234008 焦士璇       | Kafka 数据获取，机器人分析                                   |
| 181250168 薛人玮       | 协助数据库搭建，机器人分析                                   |
| 181250040 郭增嘉       | Hive 数据获取，机器人分析                                    |

<div style="page-break-after: always;"></div>

## Kafka 数据获取

Kafka消费者端配置如下：

![](https://obohe.com/i/2021/06/06/qwaoq1.png)

我们接收了所有数据，为方便存储与后续处理，在接受数据时我们将每96000存为一个txt文件，文件名为文件创建时的的时间戳：

![](https://obohe.com/i/2021/06/06/qwxl3w.png)

<div style="page-break-after: always;"></div>

## Hive 数据获取

数据库表获取主要参考了该网站专栏：https://zhuanlan.zhihu.com/p/98658806

### 搭建 Hadoop

- 配置 $HADOOP_HOME/etc/hadoop/core-site.xml

![](https://obohe.com/i/2021/06/05/10f9c5a.png)

- 配置 $HADOOP_HOME/etc/hadoop/hdfs-site.xml

![](https://obohe.com/i/2021/06/05/10fa44e.png)

- 配置公钥

![](https://obohe.com/i/2021/06/05/10fa2sb.png)

- 查看 ssh 配置

![](https://obohe.com/i/2021/06/05/10fa8pf.png)

- 格式化 namenode

![](https://obohe.com/i/2021/06/05/10fb3gg.png)

- 启动

![](https://obohe.com/i/2021/06/05/10fbq30.png)

### 搭建 Hive

- 配置 hive-site.xml

![](https://obohe.com/i/2021/06/05/10fce4i.png)

- 相关环境变量

![](https://obohe.com/i/2021/06/05/10fcljx.png)

- mysql 创建 hive 数据库并赋权

![](https://obohe.com/i/2021/06/05/10fczyt.png)

- 启动 hive

![](https://obohe.com/i/2021/06/05/10fmab6.png)

### 启动 Hiveserver2

![](https://obohe.com/i/2021/06/05/10geiee.png)

- 使用 beeline 连接到位于 172.29.4.17 的远程 hiveserver2

![](https://obohe.com/i/2021/06/05/10gguj9.png)

### 使用 sqoop 导出远程 hive 中数据到本地 mysql

- 相关配置

![](https://obohe.com/i/2021/06/05/10ghp39.png)

- 查看

![](https://obohe.com/i/2021/06/05/10gi02b.png)

- mysql 建表

![](https://obohe.com/i/2021/06/05/10gijw2.png)

- 导出

![](https://obohe.com/i/2021/06/05/10gilb8.png)

### 最终数据

![](https://obohe.com/i/2021/06/05/10gjj4g.png)

## A 部分实验报告 - 关注点分析

### 可视化

在 A 部分的展示部分，为了更好更清晰地将分析结果呈现出来，使用了网页形式做展示，使用到的技术栈如下：

- React.js 前端框架
- Ant-design UI
- Echarts

### 分析关注点

#### 产品销量分析

从购买相关的数据表中截取出销量排名前 20 的产品 id，并采用条形图进行显示，其中横坐标表示产品 id，纵坐标表示对应产品的销量。根据结果可以明显发现，销量前九的数据和后面数据有较大的数量级断层，前九的销量基本分布在 70000 以上，且相差不大，而从第 10 名以后的数据则数量级显著下降，九天以内的销量从千到百依次递减，最终维持在百数内。观察产品 id 发现，排名前九的产品 id 显然存在某种规律，因此推测此类产品可能是某类需要批量购置的产品，而十名以后的产品则相对较为常规，九天内的购买量在几千到几百不等。

- sql 语句

```sql
select item_id, count(*) from buydata group by item_id order by count(*) desc limit 20;
```

- 结果

![](https://obohe.com/i/2021/06/05/zksvro.png)

#### 交易量分析

我们统计了从 11 月 25 日到 12 月 3 号每小时内的销售产品总量，并采用折线图进行显示，从而可以大体清晰地展现出每天随著时间的变化所售出产品的总销量，进而对用户的活跃时间进行推测，为后续的决策提供一定的参考。由图可以明显看出电商平台的产品销量随著时间变化存在一定的规律——即每天用户购买的活跃时间相对固定。0 点过后，由于作息等客观原因，用户购买行为基本呈减小的趋势，这种趋势一直维持到早上六点左右，之后随著用户的醒来，销售量开始回升，并在中午 11:00-12:00 左右达到巅峰，12 点之后会有回落，这种降低趋势一直持续到 18:00 左右，之后销售量会再次上升，21:00—22:00 左右攀升至新的顶峰，22:00 之后销售整体继续呈下降趋势，直到午夜。由上述数据走向可以看出，用户通常在中午 10-12 点以及晚上 8-10 点较为活跃，发生的购买行为比较多。

- sql 语句

```sql
select count(*) as sum, DATE_FORMAT(stamp ,'%Y-%m-%d %H:00:00') as t from buydata group by t order by t asc
```

- 结果

![](https://obohe.com/i/2021/06/05/zlvelk.png)

#### 浏览转化率分析

结合之前进行的交易量分析，以及流数据部分的浏览数据，我们用两条折线图展示了从 11 月 25 号到 12 月 3 号的销售产品量以及浏览量，所谓转化率，即在浏览以后选择购买的用户所占的比例，我们初步预期通过比较两条折线的走势以及中间的差距来评估转化率，结果发现，某一时刻浏览量与购买量相差较为悬殊，因此，直接对不同时间端的交易量/浏览量进行计算，绘制新的折线图，更为直观地展现比例。首先从第一张图可以看出，浏览量和交易量的走向大体一致，即大部分交易发生的高峰期本质上也是用户浏览的高峰期。结合转化率的走向，可以发现，午夜之后转化率较低，只有 10%左右，两个增长期分别在早上六点之后和中午 12 点之后，两个高峰分别在中午 12 点左右和晚上 10 点左右，转化率可以攀升至 35%和 45%左右。

- sql 语句

```sql
select count(*) as sum, DATE_FORMAT(time ,'%Y-%m-%d %H:00:00') as t from user where action <>
"buy" group by t order by t asc
```

- 结果

![](https://obohe.com/i/2021/06/05/zm295k.png)

![](https://obohe.com/i/2021/06/05/9w8d653.png)

## B 部分实验报告 - 分析机器人源代码

### 分析方案讨论

#### 1. 数据特征总结

- sessionID（对于本字段的理解上本人存疑，根据样本数据观察，正常情况下 sessionID 和 userID 是一对一的）

- 时间

- LogHandler（日志，应该是记录用户的数据）

  - uri（根据样本来看是以下的几种，针对不同 uri 的 requestbody 有所不同）

    - /item/getDetail（商品详细信息）
    - /item/favor（收藏）
    - /item/cart（购物车）
    - /item/buy（购买）
    - /user/login（登录）

  - request body

    userID,itemID,categoryID

    uri 中前三个对应的请求体均包含上面这三个

    特殊字段：

    - /item/buy

      除包含上述三个字段之外，还包含 isSecondKill 字段，值为 1/0，标识是否为秒杀，如果为该字段为 1，则还包含字段 success，标识此次秒杀是否成功

      示例：

      isSecondKill 为 0 时：

      ```txt
      [SESSIONID=c6cc1485a2ab735dc4e40018f4dc34a2] 2017-11-25 00:00:26 DEBUG [nio-8080-exec-1] com.some.taopao.aop.LogHandler : uri=/item/buy | requestBody={"userId" : "619456", "itemId" : "4583524", "categoryId" : "910741", "isSecondKill" : "0"}
      ```

      isSecondKill 为 1 时：

      ```
      [SESSIONID=7d6cc33cd9a75e0a09a971b5175009e8] 2017-11-25 00:00:26 DEBUG [nio-8080-exec-1] com.some.taopao.aop.LogHandler : uri=/item/buy | requestBody = {"userId" : "799821", "itemId" : "5000008", "categoryId" : "140410", "isSecondKill" : "1", "success" : "0"}
      ```

    - /user/login

      **此 uri 较为特别，对应的数据除了上述的通用的数据字段（sessionID、时间、uri、request body）之外，新增 IPADDR，用来标识用户登录的 ip 地址**

      requst body 包含的内容为 userID, password,authCode,success，前三个都是单纯的的数字或数字字母组合，最后一个值为 1/0，标识是否成功

      示例

      ```
      [IPADDR=182.89.121.186] [SESSIONID=efc69661be13a0c8d8e9b893e93714ad] 2017-11-25 00:00:27 DEBUG [nio-8080-exec-1] com.some.taopao.aop.LogHandler : uri=/user/login | requestBody = {"userId" : "967782", "password" : "7755d3556172022856c93b2fe7b682c8", "authCode" : "3dd8f8df5e7161446914c66c01134894", "success" : "1"}
      ```

#### 2. 分析思路（针对上述字段）

首先，个人认为可以针对全部数据，进行一般用户行为的统计和分析，从而获取一般用户在进行一系列操作的频率或数量，作为后续判定机器人的重要参考标准

以下思路仅供参考，某些描述可能很抽象且跟实际分析操作存在差别

##### 2.1 分析撞库

助教描述：

- 产生大量登录请求，同一 IP 可能尝试了多个帐号
- 登录完成后行为很少
- 可能会经常给出错误的 authCode 或者 password，导致 sessionId 经常更换

思路：

需要的数据包括：uri 为/user/login 的数据

根据描述，检测的指标可以包括：IPADDR 与 sessionID、userID 之间的映射（即判此 IP 是否对应多个账号）、是否在短期内进行频繁的登录操作，即 success 经常为 0，符合某项特征的为怀疑对象，如果同时满足两条（频繁登录，经常失败且经常变更 sessionID）为重点怀疑对象

针对以上怀疑对象，筛选出最终成功登录的（即 success 为 1）的数据之后统计这些对象登录后的行为，即/cart、/favor、/getDetail，如果行为过于单一（如重复进行浏览商品信息），也为重点怀疑对象

##### 2.2 分析抢单

助教描述

- 抢购成功率较高
- 一般在**整点整附近**进行抢购

思路：

需要的数据：整点时间是重要分析指标，从数据中筛选出时间在整点附近（具体范围待定，比如 5 分钟以内），uri 为/item/buy，isSecondKill 为 1 的数据，之后针对这一部分数据进行分析操作

计算成功率：在以上数据的基础之上，计算不同 userID/sessionID 的秒杀成功率，即当前 userID 的所有秒杀数据中 success 为 1 的数据所占的比例，比例相对特别高（与其他 user 差别很大）为重点怀疑对象

##### 2.3 分析刷单

助教描述

- 对应的购买行为集中在同一商品上
- 大量短时间内重复购买请求

思路：

主要针对购买数据，即 uri 为 item/buy

关注指标：

在很短的时间内某 itemID 出现了大量购买行为，即在 uri 为 buy 的数据中，某个 itemID 在间隔较短时间内的数据中多次出现，该 itemID 对应的数据为怀疑对象，需进行筛选做后续分析

也可以观察相同 itemID 下是否存在大量相同的 userID，出现者为怀疑对象

##### 2.4 分析爬虫

助教描述

- 同一 IP 地址对应数个用户，大量顺序重复单一或数个类目下的浏览行为
- 大量短时间内重复浏览请求

思路：

主要针对 uri 为 item/getDetail 数据

首先，依旧需要将 IDADDR

分析思路与刷单类似，只不过需要观测的是某个 categoryID 下是否出现大量重复的 userID/sessionID

### 撞库机器人

- 分析思路：产生大量登录请求，同一 IP 可能尝试了多个帐号；登录完成后行为很少；可能会经常给出错误的 authCode 或者 password，导致 sessionId 经常更换
- 分析过程：
  - 筛选出尝试登录过两个以上账号的 ip 地址
    - 每个 ip 登录账号数量的中位数为 1，为了区分忘记密码的用户和撞库机器人，我们认为撞库机器人至少会尝试登录两个以上的账号
  - 在上一步的基础上，统计每个 ip 地址登录某一账号成功、失败的次数，并计算对应的失败率
  - 在上一步的基础上，筛选出失败率大于等于 0.5 账号及其对应的 ip 地址
    - 最初我们选择 0.8 作为阈值，因为在测试数据中没有出现正常用户失败率超过 0.5 的情况；但在实际筛选时，我们发现撞库机器人在第一次登录成功后，后续登录
      便不再出现反复登陆失败的问题，导致真实错误率低于我们的预期；所以结合生活实际，我们决定取 0.5 作为最终阈值
  - 在上一步基础上，检测每个账号登录后的行为数量；行为数低于 3 的账号视为低活跃度账号，如果每个 ip 登录的账号中低活跃度账号占比超过 90%，则认为该 ip 为撞库机器人
    - 每个账号登陆后的行为中位数为 3，且没有行为数为 0 的账号，因此我们认为行为少于 3 的账号为低活跃度
- 分析结果：见撞库机器人.txt，比例不超过 1%

### 抢单机器人

- 抢单机器人特征

1. 抢购成功率较高

2. 在短时间内，连续快速发起多次购买请求

3. 在一次抢购失败后，仍然发起了多次购买请求

- ”在整点整附近进行抢购“ **不是**特征，分析数据可以看到几乎所有的抢购事件都发生在整点附近

- 分析方法

从数据库中选出所有购买类型为秒杀的记录

- 根据特征 1：计算出每个用户的抢单成功率。

  - <img src="https://i.loli.net/2021/06/05/wGSWhqFmL5Tndp9.png" alt="image-20210605170216225.png" style="zoom:50%;" />

    可以看到大多数用户的成功率都小于 0.3（大多数都为 0）

- 根据特征 2 计算每个用户在内连续>=5 次在 5s 内发起过购买请求的商品数量

  - <img src="https://i.loli.net/2021/06/05/xK2usZD6opQFLJG.png" alt="image-20210605170657596.png" style="zoom:50%;" />

    可以看到大多数用户的数量都小于 3（大多数都为 0）

- 根据特征 3 计算每个用户在一次抢购失败后，仍然发起了购买请求的次数

  - <img src="https://i.loli.net/2021/06/05/jq4YTa7Qner9ESz.png" alt="image-20210605170827048.png" style="zoom:50%;" />

    可以看到大多数用户的数量都小于 5（大多数都为 0）

根据上述分析，将每个用户的成功率+商品数量 _ 0.1+失败后请求数量 _ 0.1 作为用户的评分。当评分大于 1 时，可以判断为抢单机器人。（此算法与阈值都较为宽松，可以视情况调整为更为严格的判断标准）

- 结果

根据以上的算法，可以看到 user_id = “1014980”被判断为抢单机器人。检查其行为

<img src="https://i.loli.net/2021/06/05/51CjgyDQ6iBHaG7.png" alt="image-20210605155602695.png" style="zoom:50%;" />

```python
# 数据格式
dic = {
    "用户id": {
        "商品id": [("购买时间", "是否成功"), ...]
    }
}
```
几乎可以断言此用户为抢单机器人。

### 刷单机器人

- 分析思路：对应的购买行为集中在同一商品上，大量短时间内重复购买请求
- 分析过程：
  - 查找出每个用户在每件商品上购买的次数
  - 在上一步的基础上，找到那些在同一商品上购买超过 5 次的用户 id
    - 这个标准已经够宽松，我们统计到的中位数为 1。但是标准设的太低的话，刷单太少刷单机器人没意义，最后在多次尝试下最终选择 5
  - 在上一步的基础上，找到用户 id 对应的商品 id 以及时间属性
  - 在上一步基础上，设置计数器 count，若用户 id 购买同一商品且两次购买时间小于 300s，则 count 加一，否则 count 归零继续寻找，直到 count>=4 判断其为机器人或完全找完判定其不是机器人
- 分析结果：见刷单机器人.txt，比例不超过 1%

### 爬虫机器人

- 分析思路：同一 IP 地址对应数个用户，大量顺序重复单一或数个类目下的浏览行为；大量短时间内重复浏览请求
- 分析过程：
  - 筛选出尝试登录过两个及以上账号的 ip 地址
    - 这个判断条件与撞库机器人的第一重判断条件有重合，考虑到撞库机器人几乎都符合“尝试登录过两个以上账号”这个条件，这里放宽限制
  - 在上一步的基础上，找到每个用户 id 的行为对应的时间和商品种类
  - 在上一步基础上，设置计数器 count，若用户 id 连续查看同一品类商品且两次查看时间小于 300s，则 count 加一，否则 count 归零继续寻找，直到 count>=4 判断其为机器人或完全找完判定其不是机器人
- 分析结果：见刷单机器人.txt，比例不超过 1%
